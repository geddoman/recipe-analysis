{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ____Text Preprocessing Section_____#\n",
    "\"\"\"\n",
    "Some spices and herbs (From hereon I use the term interchangeably)\n",
    "can be expressed as 2 or 3 word phrases (such as \"black pepper\")\n",
    "they also can have multiple variations (such as \"ground black pepper\" or \n",
    "\"crushed black pepper\", dismissing the fact that ground and crushed \n",
    "black pepper would be considered by many as 2 completely different\n",
    "spices each with their own use.For my examples and experiments I consider\n",
    "them the same.\n",
    "\n",
    "So here we have a simple problem, to achieve better results \n",
    "(regardless of the experiment) we want to group all the variations of each\n",
    "spice or herb under one term. One way of doing so is to choose a term \n",
    "(good practice is to choose the most common representation) and replace \n",
    "all other variants with the chosen term. Almost all of the algorithms and \n",
    "methods we'll use do not detect 2 or 3 word phrases on their own, the \n",
    "easiest method is to use an underscore to replace the space in these terms \n",
    "so we will have \"black_pepper\" which to our algorithms is considered\n",
    "as one word, yet we still preserve the unique combination of those\n",
    "3 words together.\n",
    "\n",
    "So my method here is simple. It focuses only on some spices and herbs,\n",
    "which I have collected from online resources and heavily edited the list\n",
    "adding the different variants manually, as well as, adding some missing spices\n",
    "(no onion powder? no dry aromatics??). The text file for spices has each\n",
    "spice in a line and always starts with the chosen word of the spice\n",
    "(black_pepper) followed by a tab and then the variants separated by a comma.\n",
    "The variants should also contain the chosen term (from hereon called the key)\n",
    "but in its natural form (without underscores).\n",
    "\n",
    "This use of double delimiters makes it very easy to read this text and have it\n",
    "stored as a standard python dictionary.\n",
    "\n",
    "The preprocessing pipeline is targeted for English documents. It uses regex\n",
    "to remove any unwanted characters and to remove any added spaces. spaCy is \n",
    "used for the optional lemmatization (although it is slow), NLTK is used only\n",
    "to get the English stop word list and finally a small library called flashtext\n",
    "is used to replace the spice variants\n",
    "\n",
    "---> Should add the option to get tagged documents from text to use readily\n",
    "---> for Doc2Vec models.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Importing the necessary libraries\n",
    "\n",
    "import re\n",
    "import csv\n",
    "import pickle\n",
    "from flashtext import KeywordProcessor\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "\n",
    "\n",
    "# Assigning the replacement dict file path and instantiating the spice_dict\n",
    "\n",
    "spice_repl_file = 'spice_repl_dict.txt'\n",
    "spice_dict = {}\n",
    "\n",
    "# Loading the replacement dictionary and preparing the keyword processor\n",
    "\n",
    "spice_dict = {}\n",
    "with open(spice_repl_file) as f:\n",
    "    spice_row_reader = csv.reader(f,delimiter='\\t')\n",
    "    for key,value in spice_row_reader:\n",
    "        spice_dict[key] = value.split(',')\n",
    "    \n",
    "kp = KeywordProcessor()\n",
    "kp.add_keywords_from_dict(spice_dict)\n",
    "\n",
    "# Setting the patterns for regex used in the cleaning helper function\n",
    "\n",
    "patt = re.compile(r'[\\W+]') # Pattern for any non-alphanumeric characters\n",
    "patt_2 = re.compile(r'\\b\\d+\\b') # Pattern for stand-alone numeric characters\n",
    "patt_3 = re.compile(r'\\b[A-Za-z]\\b') # Pattern for single alphabetical characters\n",
    "patt_4 = re.compile(r' {2,}') # Pattern for repeating whitespace\n",
    "\n",
    "\n",
    "# Loading the English stop words from NLTK and assiging to a set object\n",
    "\n",
    "stop_set = set(stopwords.words('english'))\n",
    "\n",
    "# Updating the stop_set with additional terms\n",
    "stop_set.update(['-PRON-','minute','add','heat','cook','minutes'])\n",
    "\n",
    "# Loading the spacy model for use in lemmatization\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser','ner'])\n",
    "\n",
    "def text_cleaner(doc):\n",
    "    \"\"\" \n",
    "\n",
    "    Helper function that replaces ampersands '&' with 'and' then applies a\n",
    "    predefined regex pattern to remove special characters, added whitespace\n",
    "    and returns a lower case document string.\n",
    "\n",
    "    This function can be used on its own, but it is part of the more\n",
    "    encompassing text_prepro function.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    doc : str\n",
    "        The document or string to be cleaned\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    Normalized and lower case str\n",
    "\n",
    "    \"\"\"\n",
    "    doc = doc.replace('&',' and ')\n",
    "    doc = re.sub(patt,' ',doc)\n",
    "    doc = re.sub(patt_2,' ',doc)\n",
    "    doc = re.sub(patt_3,' ',doc)\n",
    "    doc = re.sub(patt_4,' ',doc)\n",
    "    return doc.lower().strip()\n",
    "\n",
    "\n",
    "def lemmatizer(doc):\n",
    "    \"\"\"\n",
    "    \n",
    "    Helper function for lemmatization of text, it uses spaCy's pre-trained\n",
    "    model, in this case it is using the standard small model. The results\n",
    "    include the string -PRON- replacing pronouns in the doc. Due to this\n",
    "    -PRON- is added to the stop list.\n",
    "\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    doc : str\n",
    "        The document or string to be lemmatized using spaCy\n",
    "\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    Str with lemmas of words where applicable\n",
    "\n",
    "    \"\"\"\n",
    "    prepared_doc = nlp(doc)\n",
    "    return ' '.join([token.lemma_ for token in prepared_doc])\n",
    "\n",
    "\n",
    "def tokenizer(doc, remove_stop=True):\n",
    "    \"\"\"\n",
    "\n",
    "    Helper function that takes a document string as an argument and tokenizes\n",
    "    the text into words that are not in the stop list. This should be used as\n",
    "    the final function after the text is cleaned.\n",
    "\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    doc : str\n",
    "        The document or string of text to be tokenized\n",
    "\n",
    "    remove_stop : boolean, default True\n",
    "        Determines whether to remove stop words or not\n",
    "\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    List of tokenized words\n",
    "\n",
    "    \"\"\"\n",
    "    if remove_stop:\n",
    "        return [word.strip() for word in doc.split() if word not in stop_set]\n",
    "    else:\n",
    "        return [word.strip() for word in doc.split()]\n",
    "\n",
    "def text_prepro(text, r_stop=True, lemmatize=True):\n",
    "    \"\"\"\n",
    "\n",
    "    Main function that preprocesses the text by:\n",
    "    1-Replacing ampersands\n",
    "    2-Removing non-alphanumeric characters and added whitespace\n",
    "    3-Removing single character words\n",
    "    4-Removing numerical characters not attached to alphabetical characters\n",
    "    5-Converting to lowercase\n",
    "    6-Replacing predefined words/phrases using a custom dictionary\n",
    "    7-Removing predefined stop words\n",
    "    8-Tokenizes the text, splitting on horizontal whitespace\n",
    "    9-Lemmatizes the text\n",
    "\n",
    "    Returns a cleaned list of tokens from the text.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "\n",
    "    text : str\n",
    "        Document or string of text to be preprocessed\n",
    "\n",
    "    r_stop : boolean, default True\n",
    "        Value to pass to the tokenizer to remove or keep stop words\n",
    "\n",
    "    lemmatize : boolean\n",
    "        Determines whether to lemmatize the text or not\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    List of cleaned, lemmatized (if chosen), and tokenized words\n",
    "    \"\"\"\n",
    "\n",
    "    text_cln = text_cleaner(text)\n",
    "    text_cln = kp.replace_keywords(text_cln)\n",
    "    if lemmatize:\n",
    "        text_cln = lemmatizer(text_cln)\n",
    "    text_cln = tokenizer(text_cln,remove_stop=r_stop)\n",
    "    return text_cln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "recipe_file = 'full_recipes.csv'\n",
    "recipe_df = pd.read_csv(recipe_file, index_col=0)\n",
    "\n",
    "# Although none would be present, it is best to check for any missing\n",
    "# Values and handle them as necessary\n",
    "\n",
    "# Cleaning the text of the recipes using the preprocessing function\n",
    "recipe_df['method_cln'] = recipe_df['method'].apply(text_prepro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_id2word = Dictionary(documents=recipe_df['method_cln'].values)\n",
    "recipe_corpus = [recipe_id2word.doc2bow(recipe) for recipe in recipe_df['method_cln'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_tfidf = TfidfModel(recipe_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_tfidf_corpus = recipe_tfidf[recipe_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_df = pd.DataFrame(index=recipe_id2word.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_df = pd.DataFrame(index=recipe_id2word.keys())\n",
    "\n",
    "keyword_df['keyword'] = ''\n",
    "keyword_df['term_frequency'] = 0\n",
    "keyword_df['document_frequency'] = 0\n",
    "keyword_df['tfidf'] = 0.0\n",
    "\n",
    "for id,word in recipe_id2word.items():\n",
    "    keyword_df.at[id,'keyword'] = word\n",
    "    \n",
    "for doc in recipe_corpus:\n",
    "    for word_id,freq in doc:\n",
    "        keyword_df.at[word_id,'term_frequency'] += freq\n",
    "    \n",
    "for k,v in recipe_tfidf.dfs.items():\n",
    "    keyword_df.at[k,'document_frequency'] = v\n",
    "    \n",
    "for doc in recipe_tfidf_corpus:\n",
    "    for word_id,tfidf_score in doc:\n",
    "        keyword_df.at[word_id,'tfidf'] += tfidf_score   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>term_frequency</th>\n",
       "      <th>document_frequency</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>apple</td>\n",
       "      <td>411</td>\n",
       "      <td>135</td>\n",
       "      <td>31.441397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    keyword  term_frequency  document_frequency      tfidf\n",
       "303   apple             411                 135  31.441397"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_df.loc[keyword_df['keyword'] == 'apple']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
